#!/usr/bin/env python3
"""
Doc Converter - æ‰¹é‡æ–‡æ¡£è½¬ Markdown å·¥å…·

ä½¿ç”¨ markitdown å°†åŠå…¬æ–‡æ¡£ï¼ˆPDFã€PPTã€Wordã€Excel ç­‰ï¼‰æ‰¹é‡è½¬æ¢ä¸º Markdownï¼Œ
é€šè¿‡æœ¬åœ° Ollama æ¨¡å‹ç”Ÿæˆæ‘˜è¦ï¼Œç”Ÿæˆ CSV ç´¢å¼•ä¾› Claude åç»­æ€ç»´å¯¼å›¾åˆ†ç±»ã€‚
"""

import argparse
import csv
import hashlib
import json
import os
import sys
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Optional


# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
SUPPORTED_EXTENSIONS = {
    ".pdf", ".pptx", ".docx", ".xlsx", ".xls",
    ".csv", ".html", ".htm", ".epub",
    ".json", ".xml",
}

# æ‰«ææ—¶æ’é™¤çš„ç›®å½•
EXCLUDED_DIRS = {
    ".git", ".svn", ".hg",
    ".cache", ".npm", ".yarn", ".pnpm",
    ".venv", "venv", "env", ".env",
    "__pycache__", ".pytest_cache",
    "node_modules", "vendor", "packages",
    "Library", ".Trash",
    ".idea", ".vscode", ".vs",
    "build", "dist", "target", "out",
    ".summaries",
}


@dataclass
class DocInfo:
    """æ–‡æ¡£ä¿¡æ¯"""
    original_path: str
    file_type: str
    file_size: int
    file_size_human: str = ""
    md5: str = ""
    is_duplicate: bool = False
    duplicate_of: str = ""
    markdown_path: str = ""
    conversion_status: str = "pending"
    error_message: str = ""
    converted_at: str = ""


@dataclass
class ConvertReport:
    """è½¬æ¢æŠ¥å‘Š"""
    scan_time: str
    source_paths: list = field(default_factory=list)
    documents: list = field(default_factory=list)
    total_files: int = 0
    total_size_bytes: int = 0
    total_size_human: str = ""
    converted_count: int = 0
    failed_count: int = 0
    summaries_dir: str = ""


def format_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.1f} PB"


def check_markitdown() -> bool:
    """æ£€æŸ¥ markitdown æ˜¯å¦å·²å®‰è£…"""
    try:
        import markitdown  # noqa: F401
        return True
    except ImportError:
        return False


# Ollama é…ç½®
OLLAMA_API_URL = "http://localhost:11434"
DEFAULT_MODEL = "qwen2.5:3b"

SUMMARY_SYSTEM_PROMPT = "ä½ æ˜¯æ–‡æ¡£æ‘˜è¦åŠ©æ‰‹ã€‚ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦ä»»ä½•æ€è€ƒè¿‡ç¨‹ï¼Œä¸è¦è¾“å‡ºå¤šä½™çš„è§£é‡Šã€‚"

SUMMARY_USER_PROMPT = """è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£å†…å®¹ç”Ÿæˆä¸€ä»½ç®€æ˜çš„ä¸­æ–‡æ‘˜è¦ï¼Œæ ¼å¼è¦æ±‚ï¼š

1. ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒå†…å®¹
2. 3-5 ä¸ªè¦ç‚¹ï¼ˆæ¯ä¸ªè¦ç‚¹ä¸€å¥è¯ï¼‰
3. 3-5 ä¸ªå…³é”®è¯

æ–‡æ¡£å: {filename}
æ–‡æ¡£ç±»å‹: {filetype}

æ–‡æ¡£å†…å®¹:
{content}

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ Markdown æ ¼å¼è¾“å‡ºï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼š

**æ ¸å¿ƒå†…å®¹**: ï¼ˆä¸€å¥è¯æ¦‚æ‹¬ï¼‰

## è¦ç‚¹

- è¦ç‚¹ 1
- è¦ç‚¹ 2
- è¦ç‚¹ 3

## å…³é”®è¯

`å…³é”®è¯1` `å…³é”®è¯2` `å…³é”®è¯3`"""

# æ‘˜è¦æ—¶æˆªå–çš„æœ€å¤§å­—ç¬¦æ•°ï¼Œé¿å…è¶…å‡ºæ¨¡å‹ä¸Šä¸‹æ–‡
MAX_CONTENT_CHARS = 8000


def check_ollama(model: str = DEFAULT_MODEL) -> tuple[bool, str]:
    """æ£€æŸ¥ Ollama æœåŠ¡å’Œæ¨¡å‹æ˜¯å¦å¯ç”¨"""
    import requests
    try:
        resp = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        if resp.status_code != 200:
            return False, "Ollama æœåŠ¡æœªå“åº”"
        models = [m["name"] for m in resp.json().get("models", [])]
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆæ”¯æŒ "qwen3:4b" åŒ¹é… "qwen3:4b" æˆ– "qwen3:4b-xxx"ï¼‰
        base_name = model.split(":")[0] if ":" in model else model
        found = any(model in m or base_name in m for m in models)
        if not found:
            return False, f"æ¨¡å‹ {model} æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: ollama pull {model}"
        return True, ""
    except requests.ConnectionError:
        return False, "Ollama æœªå¯åŠ¨ï¼Œè¯·è¿è¡Œ: ollama serve"
    except Exception as e:
        return False, f"Ollama æ£€æŸ¥å¤±è´¥: {e}"


def ollama_summarize(content: str, filename: str, filetype: str,
                     model: str = DEFAULT_MODEL) -> str:
    """ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ‘˜è¦ï¼ˆé€šè¿‡ chat API é¿å… thinking æ¨¡å¼å¹²æ‰°ï¼‰"""
    import requests
    # æˆªå–å†…å®¹é¿å…è¶…å‡ºæ¨¡å‹ä¸Šä¸‹æ–‡
    truncated = content[:MAX_CONTENT_CHARS]
    if len(content) > MAX_CONTENT_CHARS:
        truncated += f"\n\n... (å†…å®¹å·²æˆªå–å‰ {MAX_CONTENT_CHARS} å­—ç¬¦ï¼ŒåŸæ–‡å…± {len(content)} å­—ç¬¦)"

    user_prompt = SUMMARY_USER_PROMPT.format(
        filename=filename,
        filetype=filetype,
        content=truncated,
    )

    resp = requests.post(
        f"{OLLAMA_API_URL}/api/chat",
        json={
            "model": model,
            "messages": [
                {"role": "system", "content": SUMMARY_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            "stream": False,
            "options": {"temperature": 0.3, "num_predict": 1024},
        },
        timeout=120,
    )
    resp.raise_for_status()
    msg = resp.json().get("message", {})
    content = msg.get("content", "").strip()
    # Qwen3 ç­‰ thinking æ¨¡å‹å¯èƒ½æŠŠå†…å®¹æ”¾åœ¨ thinking å­—æ®µ
    if not content and msg.get("thinking"):
        content = msg["thinking"].strip()
    return content


class DocConverter:
    """æ–‡æ¡£æ‰¹é‡è½¬æ¢å™¨"""

    def __init__(self, paths: list[str]):
        self.paths = [os.path.expanduser(p) for p in paths]
        self.documents: list[DocInfo] = []

    def _should_exclude(self, path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤è¯¥è·¯å¾„"""
        for part in path.parts:
            if part in EXCLUDED_DIRS:
                return True
        return False

    def _get_summaries_dir(self) -> str:
        """è·å– .summaries è¾“å‡ºç›®å½•ï¼ˆåŸºäºç¬¬ä¸€ä¸ªè·¯å¾„ï¼‰"""
        first = Path(self.paths[0])
        if first.is_file():
            base = first.parent
        else:
            base = first
        return str(base / ".summaries")

    def scan(self) -> list[DocInfo]:
        """æ‰«ææ‰€æœ‰è·¯å¾„ï¼Œæ”¶é›†æ”¯æŒçš„æ–‡æ¡£"""
        self.documents = []

        for path_str in self.paths:
            path = Path(path_str)

            if not path.exists():
                print(f"  âš ï¸  è·¯å¾„ä¸å­˜åœ¨: {path_str}")
                continue

            if path.is_file():
                self._add_file(path)
            elif path.is_dir():
                self._scan_dir(path)

        # æŒ‰æ–‡ä»¶å¤§å°é™åºæ’åº
        self.documents.sort(key=lambda d: d.file_size, reverse=True)
        self._detect_duplicates()
        return self.documents

    def _scan_dir(self, directory: Path):
        """é€’å½’æ‰«æç›®å½•"""
        try:
            for item in sorted(directory.iterdir()):
                if item.is_dir():
                    if item.name.startswith(".") or item.name in EXCLUDED_DIRS:
                        continue
                    self._scan_dir(item)
                elif item.is_file():
                    self._add_file(item)
        except PermissionError:
            print(f"  âš ï¸  æ— æƒé™è®¿é—®: {directory}")

    def _add_file(self, path: Path):
        """æ·»åŠ å•ä¸ªæ–‡ä»¶"""
        ext = path.suffix.lower()
        if ext not in SUPPORTED_EXTENSIONS:
            return

        try:
            size = path.stat().st_size
        except OSError:
            return

        doc = DocInfo(
            original_path=str(path),
            file_type=ext,
            file_size=size,
            file_size_human=format_size(size),
            md5=self._file_md5(path),
        )
        self.documents.append(doc)

    @staticmethod
    def _file_md5(path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶ MD5"""
        h = hashlib.md5()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                h.update(chunk)
        return h.hexdigest()

    def _detect_duplicates(self):
        """é€šè¿‡ MD5 æ£€æµ‹é‡å¤æ–‡ä»¶"""
        seen: dict[str, str] = {}  # md5 -> first file path
        for doc in self.documents:
            if doc.md5 in seen:
                doc.is_duplicate = True
                doc.duplicate_of = seen[doc.md5]
            else:
                seen[doc.md5] = doc.original_path

    def preview(self, use_json: bool = False) -> ConvertReport:
        """é¢„è§ˆæ¨¡å¼ï¼šåˆ—å‡ºå°†è¦è½¬æ¢çš„æ–‡æ¡£"""
        if not self.documents:
            self.scan()

        report = ConvertReport(
            scan_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            source_paths=self.paths,
            documents=self.documents,
            total_files=len(self.documents),
            total_size_bytes=sum(d.file_size for d in self.documents),
            summaries_dir=self._get_summaries_dir(),
        )
        report.total_size_human = format_size(report.total_size_bytes)

        # é‡å¤æ–‡ä»¶ç»Ÿè®¡
        duplicates = [d for d in self.documents if d.is_duplicate]

        if use_json:
            print(json.dumps({
                "scan_time": report.scan_time,
                "source_paths": report.source_paths,
                "total_files": report.total_files,
                "total_size": report.total_size_human,
                "duplicates": len(duplicates),
                "summaries_dir": report.summaries_dir,
                "documents": [
                    {
                        "path": d.original_path,
                        "type": d.file_type,
                        "size": d.file_size_human,
                        "size_bytes": d.file_size,
                        "md5": d.md5,
                        "is_duplicate": d.is_duplicate,
                        "duplicate_of": d.duplicate_of,
                    }
                    for d in self.documents
                ],
            }, indent=2, ensure_ascii=False))
        else:
            print(f"ğŸ“‚ æ‰«æè·¯å¾„: {', '.join(self.paths)}")
            print(f"ğŸ“Š æ‰¾åˆ° {report.total_files} ä¸ªæ–‡æ¡£, å…± {report.total_size_human}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {report.summaries_dir}")
            print("")

            # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„ç»Ÿè®¡
            by_type: dict[str, list[DocInfo]] = {}
            for doc in self.documents:
                by_type.setdefault(doc.file_type, []).append(doc)

            type_icons = {
                ".pdf": "ğŸ“„", ".pptx": "ğŸ“Š", ".docx": "ğŸ“",
                ".xlsx": "ğŸ“ˆ", ".xls": "ğŸ“ˆ", ".csv": "ğŸ“ˆ",
                ".html": "ğŸŒ", ".htm": "ğŸŒ", ".epub": "ğŸ“š",
                ".json": "ğŸ“‹", ".xml": "ğŸ“‹",
            }

            for ext, docs in sorted(by_type.items(), key=lambda x: -len(x[1])):
                icon = type_icons.get(ext, "ğŸ“„")
                total_size = sum(d.file_size for d in docs)
                print(f"  {icon} {ext}: {len(docs)} ä¸ª, {format_size(total_size)}")

            if self.documents:
                print("")
                print("ğŸ“‹ æ–‡ä»¶åˆ—è¡¨:")
                for i, doc in enumerate(self.documents, 1):
                    icon = type_icons.get(doc.file_type, "ğŸ“„")
                    name = Path(doc.original_path).name
                    dup_tag = " âš ï¸ é‡å¤" if doc.is_duplicate else ""
                    print(f"  {i:3d}. {icon} {name} ({doc.file_size_human}){dup_tag}")

            # å±•ç¤ºé‡å¤æ–‡ä»¶è¯¦æƒ…
            if duplicates:
                print("")
                print(f"âš ï¸  å‘ç° {len(duplicates)} ä¸ªé‡å¤æ–‡ä»¶ (MD5 ç›¸åŒ):")
                for doc in duplicates:
                    dup_name = Path(doc.original_path).name
                    orig_name = Path(doc.duplicate_of).name
                    print(f"  - {dup_name} == {orig_name}  [{doc.md5[:8]}...]")

        return report

    def convert(self, use_json: bool = False) -> ConvertReport:
        """æ‰§è¡Œè½¬æ¢"""
        if not self.documents:
            self.scan()

        if not self.documents:
            print("â„¹ï¸  æ²¡æœ‰å¯è½¬æ¢çš„æ–‡æ¡£")
            return ConvertReport(
                scan_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                source_paths=self.paths,
            )

        # æ£€æŸ¥ markitdown
        if not check_markitdown():
            print("âŒ markitdown æœªå®‰è£…")
            print("   è¯·è¿è¡Œ: pip install 'markitdown[all]'")
            sys.exit(1)

        from markitdown import MarkItDown

        md_converter = MarkItDown()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        summaries_dir = self._get_summaries_dir()
        converted_dir = os.path.join(summaries_dir, "converted")
        briefs_dir = os.path.join(summaries_dir, "briefs")
        os.makedirs(converted_dir, exist_ok=True)
        os.makedirs(briefs_dir, exist_ok=True)

        report = ConvertReport(
            scan_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            source_paths=self.paths,
            documents=self.documents,
            total_files=len(self.documents),
            total_size_bytes=sum(d.file_size for d in self.documents),
            summaries_dir=summaries_dir,
        )
        report.total_size_human = format_size(report.total_size_bytes)

        if not use_json:
            print(f"ğŸ”„ å¼€å§‹è½¬æ¢ {report.total_files} ä¸ªæ–‡æ¡£...")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {summaries_dir}")
            print("")

        for i, doc in enumerate(self.documents, 1):
            name = Path(doc.original_path).stem
            ext = doc.file_type
            md_filename = f"{name}{ext}.md"
            md_path = os.path.join(converted_dir, md_filename)

            if not use_json:
                print(f"  [{i}/{report.total_files}] è½¬æ¢: {Path(doc.original_path).name}...", end=" ")

            # è·³è¿‡é‡å¤æ–‡ä»¶
            if doc.is_duplicate:
                doc.conversion_status = "skipped_duplicate"
                if not use_json:
                    print(f"â­ï¸  é‡å¤ (åŒ {Path(doc.duplicate_of).name})")
                continue

            try:
                result = md_converter.convert(doc.original_path)
                content = result.text_content if result.text_content else ""

                with open(md_path, "w", encoding="utf-8") as f:
                    f.write(f"# {name}\n\n")
                    f.write(f"> åŸå§‹æ–‡ä»¶: {doc.original_path}\n")
                    f.write(f"> æ–‡ä»¶ç±»å‹: {ext} | å¤§å°: {doc.file_size_human}\n")
                    f.write(f"> è½¬æ¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                    f.write("---\n\n")
                    f.write(content)

                doc.markdown_path = md_path
                doc.conversion_status = "success"
                doc.converted_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                report.converted_count += 1

                if not use_json:
                    print("âœ…")

            except Exception as e:
                doc.conversion_status = "failed"
                doc.error_message = str(e)
                report.failed_count += 1

                if not use_json:
                    print(f"âŒ {e}")

        # ç”Ÿæˆ CSV ç´¢å¼•
        csv_path = os.path.join(summaries_dir, "index.csv")
        self._write_csv(csv_path)

        if use_json:
            print(json.dumps({
                "scan_time": report.scan_time,
                "source_paths": report.source_paths,
                "total_files": report.total_files,
                "total_size": report.total_size_human,
                "converted": report.converted_count,
                "failed": report.failed_count,
                "summaries_dir": summaries_dir,
                "index_csv": csv_path,
                "documents": [
                    {
                        "original_path": d.original_path,
                        "markdown_path": d.markdown_path,
                        "file_type": d.file_type,
                        "size": d.file_size_human,
                        "status": d.conversion_status,
                        "error": d.error_message,
                    }
                    for d in self.documents
                ],
            }, indent=2, ensure_ascii=False))
        else:
            print("")
            print(f"âœ… è½¬æ¢å®Œæˆ: {report.converted_count} æˆåŠŸ, {report.failed_count} å¤±è´¥")
            print(f"ğŸ“„ CSV ç´¢å¼•: {csv_path}")
            print(f"ğŸ“ Markdown æ–‡ä»¶: {converted_dir}")
            print(f"ğŸ“ æ‘˜è¦ç›®å½•ï¼ˆå¾… Claude ç”Ÿæˆï¼‰: {briefs_dir}")

        return report

    def summarize(self, model: str = DEFAULT_MODEL,
                  use_json: bool = False) -> dict:
        """ä½¿ç”¨ Ollama ä¸ºå·²è½¬æ¢çš„ Markdown æ–‡ä»¶ç”Ÿæˆæ‘˜è¦"""
        summaries_dir = self._get_summaries_dir()
        converted_dir = os.path.join(summaries_dir, "converted")
        briefs_dir = os.path.join(summaries_dir, "briefs")
        os.makedirs(briefs_dir, exist_ok=True)

        # æ”¶é›†å·²è½¬æ¢çš„ md æ–‡ä»¶
        if not os.path.isdir(converted_dir):
            msg = f"è½¬æ¢ç›®å½•ä¸å­˜åœ¨: {converted_dir}ï¼Œè¯·å…ˆè¿è¡Œ --convert"
            if use_json:
                print(json.dumps({"error": msg}, ensure_ascii=False))
            else:
                print(f"âŒ {msg}")
            return {"error": msg}

        md_files = sorted(Path(converted_dir).glob("*.md"))
        if not md_files:
            msg = "æ²¡æœ‰æ‰¾åˆ°å·²è½¬æ¢çš„ Markdown æ–‡ä»¶"
            if use_json:
                print(json.dumps({"error": msg}, ensure_ascii=False))
            else:
                print(f"â„¹ï¸  {msg}")
            return {"error": msg}

        if not use_json:
            print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ {model} ç”Ÿæˆæ‘˜è¦...")
            print(f"ğŸ“ æ‘˜è¦è¾“å‡º: {briefs_dir}")
            print(f"ğŸ“„ å¾…å¤„ç†: {len(md_files)} ä¸ªæ–‡ä»¶")
            print("")

        results = []
        success_count = 0
        fail_count = 0

        for i, md_file in enumerate(md_files, 1):
            name = md_file.stem  # e.g. "report.pdf"
            brief_path = os.path.join(briefs_dir, f"{name}.brief.md")

            if not use_json:
                print(f"  [{i}/{len(md_files)}] æ‘˜è¦: {md_file.name}...", end=" ",
                      flush=True)

            try:
                content = md_file.read_text(encoding="utf-8")
                # è·³è¿‡ç©ºå†…å®¹
                if len(content.strip()) < 50:
                    if not use_json:
                        print("â­ï¸  å†…å®¹è¿‡å°‘ï¼Œè·³è¿‡")
                    results.append({"file": md_file.name, "status": "skipped",
                                    "reason": "å†…å®¹è¿‡å°‘"})
                    continue

                # æå–æ–‡ä»¶ç±»å‹ï¼ˆä»æ–‡ä»¶åå¦‚ report.pdf.md ä¸­å– .pdfï¼‰
                parts = name.rsplit(".", 1)
                filetype = f".{parts[-1]}" if len(parts) > 1 else ""

                summary = ollama_summarize(content, name, filetype, model)

                # å†™å…¥æ‘˜è¦æ–‡ä»¶
                with open(brief_path, "w", encoding="utf-8") as f:
                    f.write(f"# {name} æ‘˜è¦\n\n")
                    f.write(f"**æ–‡ä»¶ç±»å‹**: {filetype} | ")
                    f.write(f"**æ¨¡å‹**: {model}\n\n")
                    f.write(summary)
                    f.write("\n")

                success_count += 1
                results.append({"file": md_file.name, "status": "success",
                                "brief_path": brief_path})

                if not use_json:
                    print("âœ…")

            except Exception as e:
                fail_count += 1
                results.append({"file": md_file.name, "status": "failed",
                                "error": str(e)})
                if not use_json:
                    print(f"âŒ {e}")

        summary_result = {
            "model": model,
            "total": len(md_files),
            "success": success_count,
            "failed": fail_count,
            "briefs_dir": briefs_dir,
            "results": results,
        }

        if use_json:
            print(json.dumps(summary_result, indent=2, ensure_ascii=False))
        else:
            print("")
            print(f"âœ… æ‘˜è¦å®Œæˆ: {success_count} æˆåŠŸ, {fail_count} å¤±è´¥")
            print(f"ğŸ“ æ‘˜è¦æ–‡ä»¶: {briefs_dir}")

        return summary_result

    def _write_csv(self, csv_path: str):
        """å†™å…¥ CSV ç´¢å¼•"""
        fieldnames = [
            "original_path", "markdown_path", "file_type",
            "file_size", "md5", "is_duplicate", "duplicate_of",
            "conversion_status", "error_message", "converted_at",
        ]

        with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for doc in self.documents:
                writer.writerow({
                    "original_path": doc.original_path,
                    "markdown_path": doc.markdown_path,
                    "file_type": doc.file_type,
                    "file_size": doc.file_size,
                    "md5": doc.md5,
                    "is_duplicate": doc.is_duplicate,
                    "duplicate_of": doc.duplicate_of,
                    "conversion_status": doc.conversion_status,
                    "error_message": doc.error_message,
                    "converted_at": doc.converted_at,
                })


def main():
    parser = argparse.ArgumentParser(
        description="Doc Converter - æ‰¹é‡æ–‡æ¡£è½¬ Markdown å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s ~/Documents/reports --preview              # é¢„è§ˆæ–‡æ¡£åˆ—è¡¨
  %(prog)s ~/Documents/reports --convert --confirm     # æ‰§è¡Œè½¬æ¢
  %(prog)s ~/Documents/reports --summarize             # ç”¨ Ollama ç”Ÿæˆæ‘˜è¦
  %(prog)s ~/Documents/reports --summarize --model qwen3:8b  # æŒ‡å®šæ¨¡å‹
  %(prog)s file1.pdf file2.pptx --convert --confirm    # è½¬æ¢æŒ‡å®šæ–‡ä»¶
  %(prog)s ~/Documents --preview --json                # JSON æ ¼å¼é¢„è§ˆ

æ”¯æŒæ ¼å¼: .pdf, .pptx, .docx, .xlsx, .xls, .csv, .html, .epub, .json, .xml
        """
    )

    parser.add_argument("paths", nargs="+", help="æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--preview", action="store_true", help="é¢„è§ˆæ¨¡å¼ï¼Œåˆ—å‡ºæ–‡æ¡£")
    parser.add_argument("--convert", action="store_true", help="æ‰§è¡Œè½¬æ¢")
    parser.add_argument("--summarize", action="store_true",
                        help="ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ‘˜è¦ï¼ˆéœ€å…ˆ --convertï¼‰")
    parser.add_argument("--model", default=DEFAULT_MODEL,
                        help=f"Ollama æ¨¡å‹åç§°ï¼ˆé»˜è®¤: {DEFAULT_MODEL}ï¼‰")
    parser.add_argument("--confirm", action="store_true", help="ç¡®è®¤æ‰§è¡Œï¼ˆå®‰å…¨æœºåˆ¶ï¼‰")
    parser.add_argument("--json", action="store_true", help="JSON æ ¼å¼è¾“å‡º")

    args = parser.parse_args()

    if not args.preview and not args.convert and not args.summarize:
        parser.print_help()
        return

    converter = DocConverter(args.paths)

    if args.preview:
        converter.preview(use_json=args.json)
        return

    if args.convert:
        if not args.confirm:
            print("âŒ è½¬æ¢éœ€è¦ --confirm å‚æ•°ç¡®è®¤")
            print("   è¯·å…ˆä½¿ç”¨ --preview é¢„è§ˆï¼Œç¡®è®¤åæ·»åŠ  --confirm æ‰§è¡Œè½¬æ¢")
            return

        # å…ˆæ£€æŸ¥ markitdown
        if not check_markitdown():
            print("âŒ markitdown æœªå®‰è£…")
            print("   è¯·è¿è¡Œ: pip install 'markitdown[all]'")
            sys.exit(1)

        converter.convert(use_json=args.json)

    if args.summarize:
        # æ£€æŸ¥ Ollama
        ok, err = check_ollama(args.model)
        if not ok:
            print(f"âŒ {err}")
            sys.exit(1)

        converter.summarize(model=args.model, use_json=args.json)


if __name__ == "__main__":
    main()
